{
  "title": "Backpropagation Part 5",
  "description": "In this video, I implement the formulas for \"gradient descent\" and adjust the bias in the train() function of my \"toy\" JavaScript neural network library. I also test the library with a simple XOR dataset.",
  "videoId": "tlqinMNM4xs",
  "nebulaSlug": "",
  "date": "2018-02-07",
  "languages": ["JavaScript"],
  "topics": ["neural network", "backpropagation", "XOR"],
  "canContribute": true,
  "timestamps": [
    {
      "time": "0:00",
      "title": "Introduction"
    },
    {
      "time": "0:14",
      "title": "Fix some mixtakes"
    },
    {
      "time": "2:29",
      "title": "Add the deltas for the bias"
    },
    {
      "time": "3:54",
      "title": "Adjust the bias by its deltas"
    },
    {
      "time": "5:57",
      "title": "Stochastic gradient descent"
    },
    {
      "time": "7:26",
      "title": "Architecture for XOR problem"
    },
    {
      "time": "8:15",
      "title": "Training data"
    },
    {
      "time": "13:05",
      "title": "Randomize the training data"
    },
    {
      "time": "13:38",
      "title": "Start training"
    },
    {
      "time": "14:19",
      "title": "Outro"
    }
  ],
  "credits": [
    {
      "title": "Editing",
      "name": "Mathieu Blanchette"
    },
    {
      "title": "Animations",
      "name": "Jason Heglund"
    }
  ]
}
